40000/650
150/40000
1/0.00375
1/0.0013
1/6
2000+1750
3750/2
#apply lik.fun to every p for fixed data
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
lik.norm.const <- sum(bin.lik.vec*interval.width)  #0.0357 (does not sum to 1)
norm.lik <- bin.lik.vec/lik.norm.const
1/(n+1)   #0.0357  Have you seen this number before?  How does this relate to using Beta pdf for likelihood?
lines(p.vec, norm.lik, lty=3, col=5, lwd=3)  #add to previous plot
##STAT 532:  A Beta-Binomial Tutorial
# A researcher is interested in learning about sleeping behavior of graduate
# students at her university.  Prior to surveying a random sample of grad students,
# she does some research on the subject.  She finds several journal articles with
#  survey results from convenience and random samples.  The surveys report anywhere
#  from 10 % to 40% of graduate students get at least 8 hours of sleep on average
#  during a week.  She also has about 10 friends at other universities who are graduate
#  students and definitely do not get an average of 8 hours of sleep over
#  the course of a week.
# She plans to randomly sample 27 graduate students from her university and ask how
#   many of them slept at least 8 hours per night on average during the last work week.
#   She wants to make inference about the proportion
#   of grad students at her university that sleep at least 8 hours per night on average (p)
# She recognizes that a Binomial likelihood would be useful.  Here is a function to calculate
#  the likelihood for a value of p given a particular set of observed data
#  (y=# successes and n=total number of trials)
lik.fun <- function(p, y, n) {
#out <- p^(y)*((1-p)^(n-y))
out <- dbinom(y,n,p)
return(out)
}
# She plans to do a Bayesian analysis and needs to specify a prior distribution for p:
# She decides to use the conjugate Beta prior - and wants to choose parameter values that
#  correspond to her prior knowledge.  Based on other articles, etc. she believes that
#  (1) the proportion is equally likely to be smaller or larger than 0.3 AND
#  (2) the probability that p is less than 0.5 is 90%.
# Let's first think about (2) by itself and find an a and b(Beta(a, b))
#   that will satisfy 2
#a.vals <- seq(0.001, 10, length=200)
#b.vals <- seq(0.001, 10, length=200)
#ab.grid <- expand.grid(a.vals, b.vals)
#ab.out <- apply(ab.grid, 1, function(ab.vec) {qbeta(.90,ab.vec[1], ab.vec[2])})
a.vals <- seq(0.001, 10, length=200)
b.vals <- seq(0.001, 10, length=200)
ab.grid <- expand.grid(a.vals, b.vals)
ab.out <- apply(ab.grid, 1, function(ab.vec) {qbeta(.90,ab.vec[1], ab.vec[2])}) # at each row apply the function qbeta function with alpha = col1 and beta = col2
#ab.out are all the p parameter values where the cdf(p) = .9
#so any value less than p will be below the 90 percentile
dist.0.5 <- abs(ab.out - 0.5) # how far ab.out is from 0.5
ab.grid[dist.0.5==min(dist.0.5),] # this the a and b value where at p = 0.5, the cdf(p)=0.9
curve(dbeta(x,4.98, 9.70))
abline(v=0.5)
qbeta(.9, 4.98, 9.70)
# Now, let's incorporate both (1) and (2) together to find an appropriate alpha and beta
#Let's just define a loss function to make it easier
loss.fun <- function(ab.vec, quant.9=0.5, quant.5=0.3) { #the proportion is equally likely to be smaller or larger than 0.3
loss.1 <- abs(qbeta(.90, ab.vec[1], ab.vec[2]) - quant.9) #find the 90% cdf(p) and subtract 0.5 find how far you are from the 0.5 mark
loss.2 <- abs(qbeta(.5, ab.vec[1], ab.vec[2]) - quant.5) #find the 50% cdf(p) and subtract 0.3 to find how far you are from the 0.3
loss <- loss.1 + loss.2 #p for half of 90 percentile + p for the 50 percentile -0.3
return(loss)            #in other words, we are trying to find the distance the given a and b parameters are from our specification
}
a.vals <- seq(0.001, 10, length=200)
b.vals <- seq(0.001, 10, length=200)
ab.grid <- expand.grid(a.vals, b.vals)
ab.loss.out <- apply(ab.grid, 1, loss.fun)
ab.grid[ab.loss.out==min(ab.loss.out),] 3.267005 7.186211 #find the a and b where
a <- 3.27
b <- 7.19
curve(dbeta(x,a,b))
abline(v=c(0.3,0.5), col=c(2,4))
qbeta(c(.5,.9), a, b)  #0.3002964 0.5002003  very close!
#we have now solved for the parameters that fit our prior knowledge
#Now, she collects her data.  From a random sample of 27 grad students, 11 said they slept on average 8 hours a night
#  over the last work week
#Now that we have data and a prior distribution specified, we can get a posterior distribution.  We will
#  do it a few different ways for illustration
#Method 1:  Analytically - The Beta-Binomial model gives us the posterior distribution as
#            Beta(a+11, b+16) = Beta(3.27+11, 7.19+16) = Beta(14.26, 23.19)
#Plot the prior, posterior, and likelihood on same plot
a <- 3.27
b <- 7.19
y <- 11  #number successes
n <- 27  #total number of trial
curve(dbeta(x,a,b), from=0, to=1, xlab="p", ylab="Density", lty=2, lwd=3, col=2, ylim=c(0,5))
curve(dbeta(x,y+1,n-y+1), add=TRUE, lty=2, lwd=3, col="purple")  #Why can I use dbeta to get likelihood?
curve(dbeta(x,a+y, b+n-y), add=TRUE, lty=1, lwd=3, col="darkgray")
legend(0.7, 4, c("Prior", "Likelihood","Posterior"), lty=c(2,2,1), col=c(2,"purple","darkgray"), lwd=c(3,3,3))
#Method 2: Computationally- we define a function to evaluate the likelihood over a grid of values for p,
#            and a function to evaluate the prior over the same grid of values for p
#apply lik.fun to every p for fixed data
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
lik.norm.const <- sum(bin.lik.vec*interval.width)  #0.0357 (does not sum to 1)
norm.lik <- bin.lik.vec/lik.norm.const
1/(n+1)   #0.0357  Have you seen this number before?  How does this relate to using Beta pdf for likelihood?
lines(p.vec, norm.lik, lty=3, col=5, lwd=3)  #add to previous plot
plot(bin.lik.vec)
dbinom(11,27,0.5)
help(dbinom)
15000/200
10000/2
##STAT 532:  A Beta-Binomial Tutorial
# A researcher is interested in learning about sleeping behavior of graduate
# students at her university.  Prior to surveying a random sample of grad students,
# she does some research on the subject.  She finds several journal articles with
#  survey results from convenience and random samples.  The surveys report anywhere
#  from 10 % to 40% of graduate students get at least 8 hours of sleep on average
#  during a week.  She also has about 10 friends at other universities who are graduate
#  students and definitely do not get an average of 8 hours of sleep over
#  the course of a week.
# She plans to randomly sample 27 graduate students from her university and ask how
#   many of them slept at least 8 hours per night on average during the last work week.
#   She wants to make inference about the proportion
#   of grad students at her university that sleep at least 8 hours per night on average (p)
# She recognizes that a Binomial likelihood would be useful.  Here is a function to calculate
#  the likelihood for a value of p given a particular set of observed data
#  (y=# successes and n=total number of trials)
lik.fun <- function(p, y, n) {
#out <- p^(y)*((1-p)^(n-y))
out <- dbinom(y,n,p)
return(out)
}
# She plans to do a Bayesian analysis and needs to specify a prior distribution for p:
# She decides to use the conjugate Beta prior - and wants to choose parameter values that
#  correspond to her prior knowledge.  Based on other articles, etc. she believes that
#  (1) the proportion is equally likely to be smaller or larger than 0.3 AND
#  (2) the probability that p is less than 0.5 is 90%.
# Let's first think about (2) by itself and find an a and b(Beta(a, b))
#   that will satisfy 2
#a.vals <- seq(0.001, 10, length=200)
#b.vals <- seq(0.001, 10, length=200)
#ab.grid <- expand.grid(a.vals, b.vals)
#ab.out <- apply(ab.grid, 1, function(ab.vec) {qbeta(.90,ab.vec[1], ab.vec[2])})
a.vals <- seq(0.001, 10, length=200)
b.vals <- seq(0.001, 10, length=200)
ab.grid <- expand.grid(a.vals, b.vals)
ab.out <- apply(ab.grid, 1, function(ab.vec) {qbeta(.90,ab.vec[1], ab.vec[2])}) # at each row apply the function qbeta function with alpha = col1 and beta = col2
#ab.out are all the p parameter values where the cdf(p) = .9
#so any value less than p will be below the 90 percentile
dist.0.5 <- abs(ab.out - 0.5) # how far ab.out is from 0.5
ab.grid[dist.0.5==min(dist.0.5),] # this the a and b value where at p = 0.5, the cdf(p)=0.9
curve(dbeta(x,4.98, 9.70))
abline(v=0.5)
qbeta(.9, 4.98, 9.70)
# Now, let's incorporate both (1) and (2) together to find an appropriate alpha and beta
#Let's just define a loss function to make it easier
loss.fun <- function(ab.vec, quant.9=0.5, quant.5=0.3) { #the proportion is equally likely to be smaller or larger than 0.3
loss.1 <- abs(qbeta(.90, ab.vec[1], ab.vec[2]) - quant.9) #find the 90% cdf(p) and subtract 0.5 find how far you are from the 0.5 mark
loss.2 <- abs(qbeta(.5, ab.vec[1], ab.vec[2]) - quant.5) #find the 50% cdf(p) and subtract 0.3 to find how far you are from the 0.3
loss <- loss.1 + loss.2 #p for half of 90 percentile + p for the 50 percentile -0.3
return(loss)            #in other words, we are trying to find the distance the given a and b parameters are from our specification
}
a.vals <- seq(0.001, 10, length=200)
b.vals <- seq(0.001, 10, length=200)
ab.grid <- expand.grid(a.vals, b.vals)
ab.loss.out <- apply(ab.grid, 1, loss.fun)
ab.grid[ab.loss.out==min(ab.loss.out),] 3.267005 7.186211 #find the a and b where
a <- 3.27
b <- 7.19
curve(dbeta(x,a,b))
abline(v=c(0.3,0.5), col=c(2,4))
qbeta(c(.5,.9), a, b)  #0.3002964 0.5002003  very close!
#we have now solved for the parameters that fit our prior knowledge
#Now, she collects her data.  From a random sample of 27 grad students, 11 said they slept on average 8 hours a night
#  over the last work week
#Now that we have data and a prior distribution specified, we can get a posterior distribution.  We will
#  do it a few different ways for illustration
#Method 1:  Analytically - The Beta-Binomial model gives us the posterior distribution as
#            Beta(a+11, b+16) = Beta(3.27+11, 7.19+16) = Beta(14.26, 23.19)
#Plot the prior, posterior, and likelihood on same plot
a <- 3.27
b <- 7.19
y <- 11  #number successes
n <- 27  #total number of trial
curve(dbeta(x,a,b), from=0, to=1, xlab="p", ylab="Density", lty=2, lwd=3, col=2, ylim=c(0,5))
curve(dbeta(x,y+1,n-y+1), add=TRUE, lty=2, lwd=3, col="purple")  #Why can I use dbeta to get likelihood?
curve(dbeta(x,a+y, b+n-y), add=TRUE, lty=1, lwd=3, col="darkgray")
legend(0.7, 4, c("Prior", "Likelihood","Posterior"), lty=c(2,2,1), col=c(2,"purple","darkgray"), lwd=c(3,3,3))
#Method 2: Computationally- we define a function to evaluate the likelihood over a grid of values for p,
#            and a function to evaluate the prior over the same grid of values for p
#apply lik.fun to every p for fixed data
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
lik.norm.const <- sum(bin.lik.vec*interval.width)  #0.0357 (does not sum to 1)
norm.lik <- bin.lik.vec/lik.norm.const
1/(n+1)   #0.0357  Have you seen this number before?  How does this relate to using Beta pdf for likelihood?
lines(p.vec, norm.lik, lty=3, col=5, lwd=3)  #add to previous plot
##Method 3: Get samples from normalized posterior
p.vec <- seq(0,1, length=500)
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)})
lik.x.prior.vec <- bin.lik.vec*prior.vec
sum.lik.x.prior <- sum(lik.x.prior.vec)
prop.to.post <- lik.x.prior.vec/(sum.lik.x.prior)
post.samples <- sample(p.vec, 10000, prob=prop.to.post, replace=TRUE) #samples proportional to weights
hist(post.samples, xlab="p", main="Posterior samples", nclass=50, col="lightgray", freq=FALSE)
curve(dbeta(x,a+y, b+n-y), add=TRUE, lty=1, lwd=3, col="magenta")
#### NOW let's use the posterior distribution to answer questions, etc.
#What is the probability that more than half the students get more than 8 hours of sleep?
1-pbeta(0.5, (a+y), (b+n-y))  #0.0693
#What is the probability that less than between 20% and 30% of students get more than 8 hours of sleep?
pbeta(0.3,(a+y), (b+n-y)) - pbeta(0.2,(a+y), (b+n-y)) #0.1469473
#90% posterior interval for p?
qbeta(c(0.05, 0.95), (a+y), (b+n-y))  #0.2556916 0.5135143
### Use simulation to get the above quantities instead of the qbeta and pbeta functions
post.samps <- rbeta(10000, (a+y), (b+n-y))
hist(post.samps, xlab="p", main="Posterior samples", nclass=50, col="lightgray", freq=FALSE)
curve(dbeta(x,(a+y), (b+n-y)), add=TRUE, col=3, lwd=2)  #overlay analytically posterior
mean(post.samps > 0.5)  #0.0693  (close to 0.0693)
quantile(post.samps, c(.05,.95))  #0.2561293 0.5124428
#### Can we use a histogram as a prior?
##  Divide the range of p into ten subintervals and then assign probabilities to the intervals
## (0,.1) (.1,.2) (.2,.3), etc.
## With weights ... 1,  5.5, 8, 7, 5, 2, 0.7, 0.1, 0, 0
midpt <- seq(0.05, 0.95, by=0.1)
prior.wts <- c(1, 5.5, 8, 7, 5, 2, 0.5, 0.1, 0, 0)
prior <- (prior.wts/sum(prior.wts))/0.1  #to actually normalize
sum(prior*.1) #check
#Don't worry about all the details of this function, but be sure to
# look at the plot
hist.prior.fun <- function(p, midpt.vec, prior.probs) {
int.size <- midpt.vec[2] - midpt.vec[1]
low <- round(10000*(midpt.vec - (int.size/2)))/10000
out <- 0 * p
for (i in 1:length(p)) {
out[i] <- prior.probs[sum(p[i] >= low)]
}
return(out)
}
p.vec <- seq(0,1, length=500)
prior.out <- hist.prior.fun(p.vec, midpt, prior)
plot(p.vec, prior.out, type="l")
curve(hist.prior.fun(x,midpt,prior), lwd=2, col=2)
#Now multiply the likelihood evaluated over the same vector of theta values
bin.beta.lik.vec <- apply(cbind(p.vec), 1, function(x) {dbeta(x, 11+1, 16+1)})
lik.x.histprior <- bin.beta.lik.vec*prior.out
int.width <- p.vec[2]-p.vec[1]
norm.const2 <- sum(lik.x.histprior*int.width)
post.histprior <- lik.x.histprior/norm.const2
plot(p.vec, prior.out, type="n", ylim=c(0,8), ylab="Density", xlab="p")
lines(p.vec, prior.out, lwd=3, col="purple") #prior
curve(dbeta(x, 12, 17), add=TRUE, lwd=2, col=4)  #Likelihood (normalized)
lines(p.vec, post.histprior, lwd=2, col=2) #Posterior
legend(0.7, 4, legend=c("Prior", "Likelihood", "Posterior"), lwd=c(2,2,2), col=c("purple", 4, 2))
## Get same summaries using this new posterior
post.hist.samps <- sample(p.vec, 10000, post.histprior, replace=TRUE)
mean(post.hist.samps) #.3849
mean(post.hist.samps > 0.5)  #0.0563  compared to 0.0693
quantile(post.hist.samps, c(.05,.95))  #.2585170 0.5070140 compared to 0.2561293 0.5124428
#What if we are interested in the distribution of NEW samples we might take from the same population
#  To investigate this we use A PREDICTIVE distribution.  We can think about PRIOR predictive distributions,
#  for which the data are not considered yet, and we can think about POSTERIOR predictive distributions which
#  use information in the observed data.
#    Suppose we want to predict new data for another sample of size 27 BEFORE data are used
#      to get draw from PRIOR PREDICTIVE DISTRIBUTION
#Step 1:  Obtain a random draw of p from its prior distribution
p.draw <- rbeta(1,a,b)
#Step 2:  Obtain a random draw from Binomial distribution with p.draw as the value for p
y.tilde.draw <- rbinom(1, size=27, p.draw)
#To actually get a distribution (which will be discrete) we want to do this many times
# this is incorporating our uncertainty in p into our uncertainty about future values of y
#  We can do this using vectors, rather than loops
p.draws <- rbeta(1000,a,b)
y.tilde.draws <- rbinom(1000, size=27, p.draws)
## Table and plot the resulting distribution
tab.y <- table(y.tilde.draws)
y.vals <- as.integer(names(tab.y))
prior.pred.probs <- tab.y/sum(tab.y)
plot(y.vals, prior.pred.probs, type="h", xlab="y", ylab="Prior Predictive Probabilities")
#    Suppose we want to predict new data for another sample of size 27 AFTER data are considered
#      to get draw from POSTERIOR PREDICTIVE DISTRIBUTION
post.p.draws <- rbeta(1000,a+y, b+n-y) #analytical posterior distribution
y.tilde.post.draws <- rbinom(1000, size=27, post.p.draws)
## Table and plot the resulting distribution
tab.post.y <- table(y.tilde.post.draws)
y.post.vals <- as.integer(names(tab.post.y))
post.pred.probs <- tab.post.y/sum(tab.post.y)
dev.new()
plot(y.post.vals, post.pred.probs, type="h", xlab="y", ylab="Posterior Predictive Probabilities")
### EXTRA FUNCTION NOT NEEDED
hist.prior.fun2 <- function(midpt.vec, prior.probs, n.per.int=11, plot=TRUE) {
int.size <- midpt.vec[2] - midpt.vec[1]
cut.points <- c(midpt.vec - int.size/2, (max(midpt.vec)+int.size/2))
n.bins <- length(cut.points)-1
theta.seq <- NULL
for (i in 1:n.bins) {
seq.int <- seq(cut.points[i], cut.points[i+1], length=n.per.int)
theta.seq <- c(theta.seq, seq.int)
}
prior.vals <- rep(prior, each=n.per.int)
if (plot==TRUE) {plot(theta.seq, prior.vals, type="l")}
out <- cbind(theta.seq,prior.vals)
return(out)
}
#apply lik.fun to every p for fixed data
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
lik.norm.const <- sum(bin.lik.vec*interval.width)  #0.0357 (does not sum to 1)
norm.lik <- bin.lik.vec/lik.norm.const
1/(n+1)   #0.0357  Have you seen this number before?  How does this relate to using Beta pdf for likelihood?
lines(p.vec, norm.lik, lty=3, col=5, lwd=3)  #add to previous plot
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
lik.norm.const <- sum(bin.lik.vec*interval.width)  #0.0357 (does not sum to 1)
norm.lik <- bin.lik.vec/lik.norm.const
1/(n+1)   #0.0357  Have you seen this number before?  How does this relate to using Beta pdf for likelihood?
lines(p.vec, norm.lik, lty=3, col=5, lwd=3)  #add to previous plot
p.vec <- seq(0,1, length=500)  #grid of values for p
bin.lik.vec <- apply(cbind(p.vec), 1, lik.fun, y=11, n=27)  #apply lik.fun to every p for fixed data
prior.vec <- apply(cbind(p.vec), 1, function(p) {dbeta(p,a,b)}) #apply prior fun to every p
lik.x.prior.vec <- bin.lik.vec*prior.vec   #multiply them together
interval.width <- p.vec[2]-p.vec[1]
norm.constant <- sum(lik.x.prior.vec*interval.width)
norm.constant   #0.06599051
posterior.vec <- lik.x.prior.vec/norm.constant
lines(p.vec, posterior.vec, lty=3, col="orange", lwd=3)  #add to previously made plot
lines(p.vec, bin.lik.vec, lty=1, col="magenta", lwd=1)  #Notice scale of this - doesn't integrate to 1
lik.norm.const <- sum(bin.lik.vec*interval.width)  #0.0357 (does not sum to 1)
norm.lik <- bin.lik.vec/lik.norm.const
1/(n+1)   #0.0357  Have you seen this number before?  How does this relate to using Beta pdf for likelihood?
lines(p.vec, norm.lik, lty=3, col=5, lwd=3)  #add to previous plot
plot(norm.lik)
plot(posterior.vec)
lines(norm.lik)
lines(posterior.vec)
lines(prior.vec)
lik.norm.const
1/(n+1)
ls
install.packages("rnoaa")
# or install a development version with
library(devtools)
install_github("ropensci/rnoaa")
install.package('devtools')
library(rnoaa)
library(scales)
library(ggplot2)
library(plyr)
results <- llply(urls, noaa_seaice, storepath = "~/")
names(results) <- seq(1979, 1990, 1)
final_data <- ldply(results)
library(dataone)
setwd("C:/Users/tony/OSS/Git/OSS-2014-Project/Scripts")
## R code to get the water quantity and quality data from USGS
#
# User Input:
# WATER_SAMPLE_SIZE, YEARS_BEFORE_DISTURBANCE, YEARS_AFTER_DISTURBANCE (set this in this code file see below)
# site_disturbance_data.csv (put this file in the same folder with this code)
#
# Output:
# available_sites.Rdat, unavailable_sites.Rdat
# site_meta.Rdat
# daily_discharge.Rdat, water_quality.Rdat
#
# Utility r code: checkSite.r (just put this file in the same folder with this code )
#install dataRetrieval
#install.packages("dataRetrieval",
#repos=c("http://usgs-r.github.com","http://cran.us.r-project.org"),
#dependencies=TRUE,
#type="both")
# load library
library(dataRetrieval)
source('checkSite.r')
# User Input:
WATER_SAMPLE_SIZE <- 10
YEARS_BEFORE_DISTURBANCE <- 2
YEARS_AFTER_DISTURBANCE <- 1
# read sitesID
sites <- read.table('site_disturbance_data.csv', header=TRUE, sep=',',colClasses=c("character"))
# set the data frames to store the downloaded data
available_sites <- data.frame(siteID=c())
unavailable_sites <- data.frame(siteID=c(),info=c())
daily_discharge <- data.frame()
water_quality <- data.frame()
site_meta <- data.frame()
daily_discharge_quantiles <- data.frame()
error_sites <- data.frame()
# check data availability
for (i in seq(nrow(sites))){
site_id <- sites[i,1]
dis_date <- as.Date(sites[i,2])
START_DATE <- as.POSIXlt(dis_date)
START_DATE$year <- (START_DATE$year-YEARS_BEFORE_DISTURBANCE)
START_DATE <- as.Date(START_DATE)
END_DATE <- as.POSIXlt(dis_date)
END_DATE$year <- (END_DATE$year+YEARS_AFTER_DISTURBANCE)
END_DATE <- as.Date(END_DATE)
check_info <- check_site_availability(site_id,START_DATE,END_DATE,WATER_SAMPLE_SIZE)
if ( check_info == 'available site') {
new_site <- data.frame(siteID=site_id,startDate=START_DATE,endDate=END_DATE,disDate=dis_date)
available_sites  <- rbind(available_sites,new_site)
} else {
new_site <- data.frame(siteID=site_id,info=check_info,startDate=START_DATE,endDate=END_DATE,disDate=dis_date)
unavailable_sites <- rbind(unavailable_sites,new_site)
}
}
save(available_sites,file="available_sites.Rdat")
save(unavailable_sites, file='unavailable_sites.Rdat')
## The following 3 sections can be run separately
# download site meta info and write to .Rdat file
for (i in seq(nrow(available_sites))){
site_id <- available_sites[i,1]
site_info <- getSiteFileData(as.character(site_id))
site_meta <- rbind(site_meta,site_info)
}
save(site_meta,file="site_meta.Rdat")
# download discharge daily mean discharge and write to .Rdat file
for (i in seq(nrow(available_sites))){
site_id <- as.character(available_sites[i,1])
START_DATE <- as.character(available_sites[i,2])
END_DATE <- as.character(available_sites[i,3])
try_site <- tryCatch(
getDVData(site_id,'00060',START_DATE,END_DATE,convert=TRUE),# if convert=True unit will be cms
error=function(e)
{message(paste("site does not seem to exist:", site_id))
# Choose a return value in case of error
return(e)})
if(inherits(try_site, "error"))
{
error_sites<- rbind(error_sites, site_id)
next
}	# if there is an error in try site, skip
site_discharge <- getDVData(site_id,'00060',START_DATE,END_DATE,convert=TRUE)# if convert=True unit will be cms
siteID <- data.frame(siteID=rep(site_id,nrow(site_discharge)))
daily_discharge <- rbind(daily_discharge,cbind(siteID,site_discharge))
message(paste("Processed site:", site_id))}
save(daily_discharge,file="daily_discharge.Rdat")
# download quality data and write to .Rdat file
for (i in seq(nrow(available_sites))){
site_id <- as.character(available_sites[i,1])
START_DATE <- as.character(available_sites[i,2])
END_DATE <- as.character(available_sites[i,3])
site_water_quality <- getSampleData(site_id,"00618",START_DATE,END_DATE)
siteID <- data.frame(siteID=rep(site_id,nrow(site_water_quality)))
water_quality <- rbind(water_quality,cbind(siteID,site_water_quality))
}
save(water_quality,file="water_quality.Rdat")
daily_discharge
head(daily_discharge)
